{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPx9F0w3v1FLh8sWFUiONfA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshitajain0802/Matchmaking/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "2LCmHMeqrIj-",
        "outputId": "48e74e87-b069-4510-e960-7649d040f093"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0907466d-3300-425e-94bf-c8ee71bb2001\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0907466d-3300-425e-94bf-c8ee71bb2001\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 10k.json to 10k.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence-transformers faiss-cpu pandas psutil humanize\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KldMI3H4sROo",
        "outputId": "22e34acf-5ee8-4f72-b057-913e219c5c06"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (5.9.5)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.11/dist-packages (4.12.3)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.52.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install faiss-cpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p5GvMM1v_L5",
        "outputId": "c03c940f-be80-4ebc-cead-ad030840645e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpvQ9QwLq3n0",
        "outputId": "c756b8d6-cee0-4993-f652-ef5c8617257e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting scalable matching system for 10K users...\n",
            "🏢 Company filtering enabled: Users will not be matched to colleagues from the same company\n",
            "✅ Loaded 10000 users from 10k.json\n",
            "📊 Estimated batches for embedding: 157\n",
            "📁 Loading cached embeddings...\n",
            "✅ Using cached embeddings\n",
            "📁 Loading cached FAISS index...\n",
            "✅ Using cached FAISS index\n",
            "🔄 Processing 10000 users in chunks of 1000...\n",
            "🏢 Company filtering is enabled - users won't be matched to same-company colleagues\n",
            "✅ Saved 30000 matches to scalable_matches_company_filtered.csv\n",
            "📊 Company filtering analysis:\n",
            "   - Total matches: 30000\n",
            "   - Same-company matches: 0\n",
            "   - Company filter effectiveness: 100.0%\n",
            "\n",
            "=== Performance Metrics ===\n",
            "Total execution time: 12.35 seconds\n",
            "Configuration: {'batch_size': 64, 'num_matches': 3, 'embedding_cache_file': 'embeddings_cache.pkl', 'faiss_index_file': 'faiss_index.bin', 'chunk_size': 1000, 'use_gpu': True, 'approximate_search': True, 'n_probe': 64, 'n_clusters': 4096}\n",
            "\n",
            "Stage Timings:\n",
            "- data_loading: 0.12s\n",
            "- embedding_check: 0.01s\n",
            "- faiss_indexing: 0.01s\n",
            "- matching_process: 10.84s\n",
            "- results_saving: 0.19s\n",
            "- analysis: 1.17s\n",
            "\n",
            "Memory Usage:\n",
            "- data_loading: 1.9 GB\n",
            "- embedding_check: 1.9 GB\n",
            "- faiss_indexing: 1.9 GB\n",
            "- matching_process: 2.0 GB\n",
            "- results_saving: 2.0 GB\n",
            "- analysis: 2.0 GB\n",
            "Peak memory usage: 2.0 GB\n",
            "\n",
            "System Info:\n",
            "- CPU cores: 2\n",
            "- Available RAM: 13.6 GB\n",
            "✅ Performance metrics saved to scalable_performance_metrics_company_filtered.csv\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import faiss\n",
        "import time\n",
        "import psutil\n",
        "import os\n",
        "import humanize\n",
        "import pickle\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import multiprocessing as mp\n",
        "from functools import partial\n",
        "import gc\n",
        "\n",
        "# === Configuration ===\n",
        "CONFIG = {\n",
        "    'batch_size': 64,  # Increased for better GPU utilization\n",
        "    'num_matches': 3,\n",
        "    'embedding_cache_file': 'embeddings_cache.pkl',\n",
        "    'faiss_index_file': 'faiss_index.bin',\n",
        "    'chunk_size': 1000,  # Process users in chunks to manage memory\n",
        "    'use_gpu': True,  # Enable GPU if available\n",
        "    'approximate_search': True,  # Use approximate search for speed\n",
        "    'n_probe': 64,  # Number of clusters to search (for IVF index)\n",
        "    'n_clusters': 4096,  # Number of clusters for IVF index\n",
        "}\n",
        "\n",
        "# === Metrics Tracking Setup ===\n",
        "metrics = {\n",
        "    'start_time': time.time(),\n",
        "    'memory_usage': [],\n",
        "    'stage_times': {}\n",
        "}\n",
        "\n",
        "def record_stage_start(stage_name):\n",
        "    metrics['stage_times'][stage_name] = {\n",
        "        'start': time.time(),\n",
        "        'end': None,\n",
        "        'duration': None\n",
        "    }\n",
        "\n",
        "def record_stage_end(stage_name):\n",
        "    if stage_name in metrics['stage_times']:\n",
        "        metrics['stage_times'][stage_name]['end'] = time.time()\n",
        "        metrics['stage_times'][stage_name]['duration'] = (\n",
        "            metrics['stage_times'][stage_name]['end'] -\n",
        "            metrics['stage_times'][stage_name]['start']\n",
        "        )\n",
        "    # Record memory usage at this stage\n",
        "    process = psutil.Process(os.getpid())\n",
        "    metrics['memory_usage'].append({\n",
        "        'stage': stage_name,\n",
        "        'memory': process.memory_info().rss\n",
        "    })\n",
        "\n",
        "def print_metrics():\n",
        "    print(\"\\n=== Performance Metrics ===\")\n",
        "    total_time = time.time() - metrics['start_time']\n",
        "    print(f\"Total execution time: {total_time:.2f} seconds\")\n",
        "    print(f\"Configuration: {CONFIG}\")\n",
        "\n",
        "    print(\"\\nStage Timings:\")\n",
        "    for stage, times in metrics['stage_times'].items():\n",
        "        if times['duration'] is not None:\n",
        "            print(f\"- {stage}: {times['duration']:.2f}s\")\n",
        "\n",
        "    if metrics['memory_usage']:\n",
        "        print(\"\\nMemory Usage:\")\n",
        "        max_memory = max(m['memory'] for m in metrics['memory_usage'])\n",
        "        for m in metrics['memory_usage']:\n",
        "            human_readable = humanize.naturalsize(m['memory'])\n",
        "            print(f\"- {m['stage']}: {human_readable}\")\n",
        "        print(f\"Peak memory usage: {humanize.naturalsize(max_memory)}\")\n",
        "\n",
        "    print(\"\\nSystem Info:\")\n",
        "    print(f\"- CPU cores: {psutil.cpu_count()}\")\n",
        "    print(f\"- Available RAM: {humanize.naturalsize(psutil.virtual_memory().total)}\")\n",
        "\n",
        "# === Step 1: Load and flatten data with chunking ===\n",
        "def repair_json_file(filename):\n",
        "    \"\"\"Attempt to repair common JSON issues\"\"\"\n",
        "    print(f\"🔧 Attempting to repair JSON file: {filename}\")\n",
        "\n",
        "    try:\n",
        "        with open(filename, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Cannot read file: {e}\")\n",
        "        return False\n",
        "\n",
        "    # Common fixes\n",
        "    original_length = len(content)\n",
        "\n",
        "    # Fix unterminated strings by finding the error location\n",
        "    try:\n",
        "        json.loads(content)\n",
        "        return True  # File is already valid\n",
        "    except json.JSONDecodeError as e:\n",
        "        error_pos = e.pos\n",
        "        print(f\"🔍 JSON error at position {error_pos}\")\n",
        "\n",
        "        # Try to fix by truncating at the error point and closing properly\n",
        "        if error_pos > 0:\n",
        "            # Find the last complete JSON object before the error\n",
        "            truncated = content[:error_pos]\n",
        "\n",
        "            # Try to close the JSON properly\n",
        "            brace_count = truncated.count('{') - truncated.count('}')\n",
        "            bracket_count = truncated.count('[') - truncated.count(']')\n",
        "\n",
        "            # Add missing closing braces/brackets\n",
        "            for _ in range(bracket_count):\n",
        "                truncated += ']'\n",
        "            for _ in range(brace_count):\n",
        "                truncated += '}'\n",
        "\n",
        "            # Test if this fixes it\n",
        "            try:\n",
        "                json.loads(truncated)\n",
        "                # Save repaired version\n",
        "                repaired_filename = filename.replace('10k.json', '_repaired.json')\n",
        "                with open(repaired_filename, 'w', encoding='utf-8') as f:\n",
        "                    f.write(truncated)\n",
        "                print(f\"✅ Repaired JSON saved as: {repaired_filename}\")\n",
        "                print(f\"📊 Original: {original_length:,} chars → Repaired: {len(truncated):,} chars\")\n",
        "                return repaired_filename\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    print(\"❌ Could not automatically repair the JSON file\")\n",
        "    return False\n",
        "\n",
        "def load_data_chunked(filename):\n",
        "    \"\"\"Load data in chunks to manage memory better\"\"\"\n",
        "    record_stage_start(\"data_loading\")\n",
        "\n",
        "    raw_data = None\n",
        "    current_filename = filename\n",
        "\n",
        "    try:\n",
        "        with open(current_filename, 'r', encoding='utf-8') as f:\n",
        "            raw_data = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ Error: '{current_filename}' file not found.\")\n",
        "        exit(1)\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"❌ Error: Invalid JSON format in '{current_filename}'\")\n",
        "        print(f\"Error details: {e}\")\n",
        "\n",
        "        # Attempt to repair the file\n",
        "        repaired_file = repair_json_file(current_filename)\n",
        "        if repaired_file:\n",
        "            try:\n",
        "                with open(repaired_file, 'r', encoding='utf-8') as f:\n",
        "                    raw_data = json.load(f)\n",
        "                    current_filename = repaired_file\n",
        "                    print(f\"✅ Successfully loaded repaired file: {repaired_file}\")\n",
        "            except Exception as repair_error:\n",
        "                print(f\"❌ Even repaired file failed: {repair_error}\")\n",
        "                exit(1)\n",
        "        else:\n",
        "            print(\"💡 Manual fix suggestions:\")\n",
        "            print(\"1. Check the file around the error position for unterminated strings\")\n",
        "            print(\"2. Ensure all quotes are properly escaped\")\n",
        "            print(\"3. Verify the file wasn't corrupted during transfer\")\n",
        "            exit(1)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Unexpected error loading JSON file: {e}\")\n",
        "        exit(1)\n",
        "\n",
        "    if raw_data is None:\n",
        "        print(\"❌ Failed to load any data\")\n",
        "        exit(1)\n",
        "\n",
        "    users = []\n",
        "    try:\n",
        "        for group in raw_data.values():\n",
        "            if isinstance(group, list):\n",
        "                users.extend(group)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing JSON structure: {e}\")\n",
        "        print(\"💡 The JSON structure might not match expected format\")\n",
        "        exit(1)\n",
        "\n",
        "    # Add index to each user\n",
        "    for idx, user in enumerate(users):\n",
        "        user['idx'] = idx\n",
        "\n",
        "    print(f\"✅ Loaded {len(users)} users from {current_filename}\")\n",
        "    print(f\"📊 Estimated batches for embedding: {(len(users) + CONFIG['batch_size'] - 1) // CONFIG['batch_size']}\")\n",
        "    record_stage_end(\"data_loading\")\n",
        "    return users\n",
        "\n",
        "# === Step 2: Helper functions ===\n",
        "def get_company_identifier(user):\n",
        "    \"\"\"Extract company/organization identifier\"\"\"\n",
        "    role = user.get(\"role\", \"\")\n",
        "    if role == \"founder\": return user.get(\"startupName\", \"\")\n",
        "    elif role == \"investor\": return user.get(\"organization\", \"\")\n",
        "    elif role == \"accelerator\": return user.get(\"name\", \"\")\n",
        "    elif role == \"legal\": return user.get(\"firm\", \"\")\n",
        "    elif role == \"government\": return user.get(\"department\", \"\")\n",
        "    elif role == \"mentors_research\": return user.get(\"institution\", \"\")\n",
        "    elif role == \"executive\": return user.get(\"company\", \"\")\n",
        "    elif role == \"mentor\": return user.get(\"name\", \"\")\n",
        "    elif role == \"cybersecurity\": return user.get(\"name\", \"\")\n",
        "    elif role == \"freelancer\": return user.get(\"name\", \"\")\n",
        "    else: return user.get(\"name\", \"\")\n",
        "\n",
        "def normalize_company_name(company_name):\n",
        "    \"\"\"Normalize company names for better matching\"\"\"\n",
        "    if not company_name or not isinstance(company_name, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Convert to lowercase and strip whitespace\n",
        "    normalized = company_name.lower().strip()\n",
        "\n",
        "    # Remove common suffixes/prefixes that might vary\n",
        "    suffixes = [' inc', ' inc.', ' llc', ' ltd', ' ltd.', ' corp', ' corp.',\n",
        "                ' company', ' co', ' co.', ' limited', ' group', ' gmbh']\n",
        "    for suffix in suffixes:\n",
        "        if normalized.endswith(suffix):\n",
        "            normalized = normalized[:-len(suffix)].strip()\n",
        "\n",
        "    return normalized\n",
        "\n",
        "def are_same_company(user1, user2):\n",
        "    \"\"\"Check if two users are from the same company\"\"\"\n",
        "    company1 = normalize_company_name(get_company_identifier(user1))\n",
        "    company2 = normalize_company_name(get_company_identifier(user2))\n",
        "\n",
        "    # If either company is empty or None, they're not considered the same company\n",
        "    if not company1 or not company2:\n",
        "        return False\n",
        "\n",
        "    # Check for exact match after normalization\n",
        "    return company1 == company2\n",
        "\n",
        "def profile_to_text(user):\n",
        "    \"\"\"Convert user profile to text representation\"\"\"\n",
        "    parts = []\n",
        "    for k, v in user.items():\n",
        "        if k == 'idx':\n",
        "            continue\n",
        "        if isinstance(v, list):\n",
        "            parts.append(f\"{k}: {', '.join(map(str, v))}\")\n",
        "        elif isinstance(v, str) and v.strip():\n",
        "            parts.append(f\"{k}: {v}\")\n",
        "    return \" | \".join(parts)\n",
        "\n",
        "# === Step 3: Embedding generation with caching ===\n",
        "def generate_embeddings(users, use_cache=True):\n",
        "    \"\"\"Generate embeddings with caching for faster reruns\"\"\"\n",
        "    record_stage_start(\"embedding_check\")\n",
        "\n",
        "    # Check if cached embeddings exist\n",
        "    if use_cache and os.path.exists(CONFIG['embedding_cache_file']):\n",
        "        print(\"📁 Loading cached embeddings...\")\n",
        "        with open(CONFIG['embedding_cache_file'], 'rb') as f:\n",
        "            cache_data = pickle.load(f)\n",
        "            if len(cache_data['embeddings']) == len(users):\n",
        "                print(\"✅ Using cached embeddings\")\n",
        "                record_stage_end(\"embedding_check\")\n",
        "                return cache_data['embeddings'], cache_data['texts']\n",
        "\n",
        "    record_stage_end(\"embedding_check\")\n",
        "\n",
        "    # Generate new embeddings\n",
        "    record_stage_start(\"text_conversion\")\n",
        "    profile_texts = [profile_to_text(u) for u in users]\n",
        "    record_stage_end(\"text_conversion\")\n",
        "\n",
        "    record_stage_start(\"model_loading\")\n",
        "    # Use smaller model for better speed-accuracy tradeoff at scale\n",
        "    model_name = \"all-MiniLM-L6-v2\"  # Faster alternative to all-mpnet-base-v2\n",
        "    model = SentenceTransformer(model_name)\n",
        "\n",
        "    # Enable GPU if available and requested\n",
        "    if CONFIG['use_gpu'] and hasattr(model, 'device'):\n",
        "        device = 'cuda' if model.device.type == 'cuda' else 'cpu'\n",
        "        print(f\"🔧 Using device: {device}\")\n",
        "    record_stage_end(\"model_loading\")\n",
        "\n",
        "    record_stage_start(\"embedding_generation\")\n",
        "    print(f\"🔄 Generating embeddings for {len(users)} users...\")\n",
        "\n",
        "    embeddings = model.encode(\n",
        "        profile_texts,\n",
        "        convert_to_numpy=True,\n",
        "        show_progress_bar=True,\n",
        "        batch_size=CONFIG['batch_size'],\n",
        "        normalize_embeddings=True  # Pre-normalize for cosine similarity\n",
        "    )\n",
        "\n",
        "    # Cache embeddings for future runs\n",
        "    cache_data = {\n",
        "        'embeddings': embeddings,\n",
        "        'texts': profile_texts,\n",
        "        'model_name': model_name,\n",
        "        'timestamp': time.time()\n",
        "    }\n",
        "    with open(CONFIG['embedding_cache_file'], 'wb') as f:\n",
        "        pickle.dump(cache_data, f)\n",
        "    print(\"💾 Embeddings cached for future runs\")\n",
        "\n",
        "    record_stage_end(\"embedding_generation\")\n",
        "    return embeddings, profile_texts\n",
        "\n",
        "# === Step 4: Build optimized FAISS index ===\n",
        "def build_faiss_index(embeddings, use_cache=True):\n",
        "    \"\"\"Build FAISS index with approximate search for scalability\"\"\"\n",
        "    record_stage_start(\"faiss_indexing\")\n",
        "\n",
        "    # Check for cached index\n",
        "    if use_cache and os.path.exists(CONFIG['faiss_index_file']):\n",
        "        print(\"📁 Loading cached FAISS index...\")\n",
        "        index = faiss.read_index(CONFIG['faiss_index_file'])\n",
        "        print(\"✅ Using cached FAISS index\")\n",
        "        record_stage_end(\"faiss_indexing\")\n",
        "        return index\n",
        "\n",
        "    dimension = embeddings.shape[1]\n",
        "\n",
        "    if CONFIG['approximate_search'] and len(embeddings) > 10000:\n",
        "        # Use IVF (Inverted File) index for approximate but fast search\n",
        "        print(f\"🏗️ Building IVF index with {CONFIG['n_clusters']} clusters...\")\n",
        "        quantizer = faiss.IndexFlatIP(dimension)\n",
        "        index = faiss.IndexIVFFlat(quantizer, dimension, CONFIG['n_clusters'])\n",
        "\n",
        "        # Train the index\n",
        "        print(\"🎯 Training index...\")\n",
        "        index.train(embeddings.astype('float32'))\n",
        "        index.add(embeddings.astype('float32'))\n",
        "        index.nprobe = CONFIG['n_probe']  # Search more clusters for better recall\n",
        "\n",
        "    else:\n",
        "        # Use exact search for smaller datasets\n",
        "        print(\"🏗️ Building exact search index...\")\n",
        "        index = faiss.IndexFlatIP(dimension)\n",
        "        index.add(embeddings.astype('float32'))\n",
        "\n",
        "    # Cache the index\n",
        "    faiss.write_index(index, CONFIG['faiss_index_file'])\n",
        "    print(\"💾 FAISS index cached for future runs\")\n",
        "\n",
        "    record_stage_end(\"faiss_indexing\")\n",
        "    return index\n",
        "\n",
        "# === Step 5: Chunked matching process with company filtering ===\n",
        "def process_user_batch(batch_data):\n",
        "    \"\"\"Process a batch of users for matching with company exclusion\"\"\"\n",
        "    batch_users, batch_indices, index, all_users = batch_data\n",
        "    batch_matches = []\n",
        "\n",
        "    for local_idx, (user_idx, user) in enumerate(zip(batch_indices, batch_users)):\n",
        "        # Get user embedding (already in index)\n",
        "        query_vec = index.reconstruct(user_idx).reshape(1, -1)\n",
        "\n",
        "        # Search for more candidates to account for company filtering\n",
        "        search_k = min(200, index.ntotal)  # Increased search size to account for filtering\n",
        "        D, I = index.search(query_vec, search_k)\n",
        "\n",
        "        final_matches = []\n",
        "\n",
        "        for sim, idx in zip(D[0], I[0]):\n",
        "            if idx == user_idx:  # Skip self\n",
        "                continue\n",
        "\n",
        "            match = all_users[idx]\n",
        "\n",
        "            # NEW: Skip users from the same company\n",
        "            if are_same_company(user, match):\n",
        "                continue\n",
        "\n",
        "            if len(final_matches) >= CONFIG['num_matches']:\n",
        "                break\n",
        "\n",
        "            final_matches.append((idx, sim))\n",
        "\n",
        "        # Store results\n",
        "        for rank, (match_idx, sim) in enumerate(final_matches, 1):\n",
        "            matched_user = all_users[match_idx]\n",
        "            batch_matches.append({\n",
        "                \"user_idx\": user_idx,\n",
        "                \"user_name\": user.get(\"name\", \"\"),\n",
        "                \"user_role\": user.get(\"role\", \"\"),\n",
        "                \"user_company\": get_company_identifier(user),\n",
        "                \"match_rank\": rank,\n",
        "                \"match_idx\": match_idx,\n",
        "                \"match_name\": matched_user.get(\"name\", \"\"),\n",
        "                \"match_role\": matched_user.get(\"role\", \"\"),\n",
        "                \"match_company\": get_company_identifier(matched_user),\n",
        "                \"similarity\": round(float(sim), 4)\n",
        "            })\n",
        "\n",
        "    return batch_matches\n",
        "\n",
        "def chunked_matching(users, index):\n",
        "    \"\"\"Process matching in chunks with parallel processing\"\"\"\n",
        "    record_stage_start(\"matching_process\")\n",
        "\n",
        "    chunk_size = CONFIG['chunk_size']\n",
        "    all_matches = []\n",
        "\n",
        "    print(f\"🔄 Processing {len(users)} users in chunks of {chunk_size}...\")\n",
        "    print(\"🏢 Company filtering is enabled - users won't be matched to same-company colleagues\")\n",
        "\n",
        "    # Process in chunks\n",
        "    batch_data_list = []\n",
        "    for i in range(0, len(users), chunk_size):\n",
        "        chunk_users = users[i:i + chunk_size]\n",
        "        chunk_indices = list(range(i, min(i + chunk_size, len(users))))\n",
        "        batch_data_list.append((chunk_users, chunk_indices, index, users))\n",
        "\n",
        "    # Use multiprocessing\n",
        "    from multiprocessing import Pool\n",
        "    with Pool() as pool:\n",
        "        results = pool.map(process_user_batch, batch_data_list)\n",
        "\n",
        "    # Flatten results\n",
        "    for chunk in results:\n",
        "        all_matches.extend(chunk)\n",
        "\n",
        "    record_stage_end(\"matching_process\")\n",
        "    return all_matches\n",
        "\n",
        "# === Main execution ===\n",
        "def main():\n",
        "    print(\"🚀 Starting scalable matching system for 10K users...\")\n",
        "    print(\"🏢 Company filtering enabled: Users will not be matched to colleagues from the same company\")\n",
        "\n",
        "    # Load data\n",
        "    users = load_data_chunked('10k.json')\n",
        "\n",
        "    # Generate embeddings\n",
        "    embeddings, profile_texts = generate_embeddings(users)\n",
        "\n",
        "    # Build FAISS index\n",
        "    index = build_faiss_index(embeddings)\n",
        "\n",
        "    # Perform matching\n",
        "    all_matches = chunked_matching(users, index)\n",
        "\n",
        "    # Save results\n",
        "    record_stage_start(\"results_saving\")\n",
        "    df = pd.DataFrame(all_matches)\n",
        "    df.to_csv(\"scalable_matches_company_filtered.csv\", index=False)\n",
        "    print(f\"✅ Saved {len(all_matches)} matches to scalable_matches_company_filtered.csv\")\n",
        "    record_stage_end(\"results_saving\")\n",
        "\n",
        "    # Additional analysis: Check company filtering effectiveness\n",
        "    record_stage_start(\"analysis\")\n",
        "    same_company_matches = 0\n",
        "    total_matches = len(all_matches)\n",
        "\n",
        "    for _, match in df.iterrows():\n",
        "        user_company = normalize_company_name(match['user_company'])\n",
        "        match_company = normalize_company_name(match['match_company'])\n",
        "        if user_company and match_company and user_company == match_company:\n",
        "            same_company_matches += 1\n",
        "\n",
        "    print(f\"📊 Company filtering analysis:\")\n",
        "    print(f\"   - Total matches: {total_matches}\")\n",
        "    print(f\"   - Same-company matches: {same_company_matches}\")\n",
        "    print(f\"   - Company filter effectiveness: {((total_matches - same_company_matches) / total_matches * 100):.1f}%\")\n",
        "    record_stage_end(\"analysis\")\n",
        "\n",
        "    # Save metrics\n",
        "    metrics_data = [\n",
        "        {'metric': 'total_users', 'value': len(users)},\n",
        "        {'metric': 'total_matches', 'value': len(all_matches)},\n",
        "        {'metric': 'same_company_matches', 'value': same_company_matches},\n",
        "        {'metric': 'company_filter_effectiveness_pct', 'value': (total_matches - same_company_matches) / total_matches * 100},\n",
        "        {'metric': 'embedding_dimensions', 'value': f\"{embeddings.shape[0]}x{embeddings.shape[1]}\"},\n",
        "        {'metric': 'model_name', 'value': \"all-MiniLM-L6-v2\"},\n",
        "        {'metric': 'total_time', 'value': time.time() - metrics['start_time']},\n",
        "        {'metric': 'approximate_search', 'value': CONFIG['approximate_search']},\n",
        "        {'metric': 'batch_size', 'value': CONFIG['batch_size']},\n",
        "        {'metric': 'company_filtering_enabled', 'value': True}\n",
        "    ]\n",
        "\n",
        "    # Add stage times\n",
        "    for stage, times in metrics['stage_times'].items():\n",
        "        if times['duration'] is not None:\n",
        "            metrics_data.append({\n",
        "                'metric': f'stage_time_{stage}',\n",
        "                'value': times['duration']\n",
        "            })\n",
        "\n",
        "    # Add memory usage\n",
        "    if metrics['memory_usage']:\n",
        "        max_memory = max(m['memory'] for m in metrics['memory_usage'])\n",
        "        metrics_data.append({\n",
        "            'metric': 'peak_memory_usage_bytes',\n",
        "            'value': max_memory\n",
        "        })\n",
        "\n",
        "    metrics_df = pd.DataFrame(metrics_data)\n",
        "    metrics_df.to_csv(\"scalable_performance_metrics_company_filtered.csv\", index=False)\n",
        "\n",
        "    print_metrics()\n",
        "    print(\"✅ Performance metrics saved to scalable_performance_metrics_company_filtered.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('scalable_performance_metrics_company_filtered.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "uAdJMmrFzLtz",
        "outputId": "3617baee-d729-4314-d3a0-136b992b7018"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8cd7b1fa-da36-41f0-88a3-e8ba9d29bbc6\", \"scalable_performance_metrics_company_filtered.csv\", 718)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}